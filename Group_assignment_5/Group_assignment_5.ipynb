{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cddfcf",
   "metadata": {},
   "source": [
    "# Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232887bd",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c33a3c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convolve2d\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m \n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import convolve2d\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126af9c1",
   "metadata": {},
   "source": [
    "## k-means algorithm via lloyd's algorithm\n",
    "* For grey images\n",
    "* based on lloyds algorithm from slides\n",
    "* the feature used must be pixel intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans_lloyd(k, image, random_seed, patience = 2):\n",
    "\n",
    "    random.seed(random_seed) # set random seed\n",
    "\n",
    "    # ectract intensity values, the relative position of the pixels does not matter\n",
    "    shape = image.shape\n",
    "    pixels = image.flatten().reshape(-1,1) # 1d array of pixels, shape (N, 1)\n",
    "    centroids = np.random.uniform(pixels.min(), pixels.max(), size = k).reshape(1, -1) # shape (1, N)\n",
    "\n",
    "\n",
    "    change_count = 0 # part of the stopping condition\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # compute distance for all points to all centroids\n",
    "        distances = np.abs(pixels - centroids) # (N,1) x (1, k) = (N , k)\n",
    "\n",
    "        #put each pixel in the cluster belonging to the nearest centroid\n",
    "        clusters = np.argmin(distances, axis= 1)\n",
    "\n",
    "        # recompute centroids (position)\n",
    "        new_centroids = np.zeros((1,k)) # empty numpy array\n",
    "        \n",
    "        for i in range(k):\n",
    "                cluster_pixels = pixels[clusters == i]\n",
    "                new_centroids[0, i] = cluster_pixels.mean()\n",
    "                    \n",
    "        \n",
    "\n",
    "        # STOPPING CONDITION\n",
    "        if np.allclose(new_centroids, centroids): # checks if the values have moved within a small margin. It returns True if the centroids have stopped moving\n",
    "            change_count += 1\n",
    "        else:\n",
    "            change_count =0\n",
    "\n",
    "        if change_count >= patience:\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    # save points to points to clusters\n",
    "    clusters = clusters.reshape(shape)\n",
    "\n",
    "    return clusters, centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9fb194",
   "metadata": {},
   "source": [
    "# Otsu thresholding algorithm\n",
    "* Based on the 1979_otsu_IEEESys.pdf document and the slides segmentation.pdf\n",
    "\n",
    "It was pretty hard to understand the algorithm, so i have written which equation from the 1979_otsu_IEEESys paper, when relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Otsu(image):\n",
    "\n",
    "    # Compute histogram\n",
    "    histogram, bin_edges = np.histogram(image, bins=256, range=(0, 256))\n",
    "\n",
    "    # Normalize historgram and regard it as a probability distribution P(i) = h(i) /N\n",
    "    p_i = histogram / np.sum(histogram) \n",
    "\n",
    "    total_mean = np.mean(image) # mean intensity of the image\n",
    "\n",
    "    max_variance = 0 # initial maximum variance\n",
    "    threshold = 0   # initial threshold\n",
    "    w0 = 0  # initial  probability for class 0\n",
    "    w1 = 0  # initial probability for class 1 \n",
    "    L = len(p_i)  # number of bins = 256\n",
    "\n",
    "    for i in range(L):\n",
    "        w0 += p_i[i]  #  probability for class 0 cumulative (equation 2)\n",
    "        w1 = 1 - w0  #  probability for class 1 (equation 3)\n",
    "\n",
    "        if w0 == 0 or w1 == 0:\n",
    "            continue\n",
    "\n",
    "        mu_k = i * p_i[i]  # cumulative mean up to bin i\n",
    "\n",
    "        mu0 =  mu_k / w0  # mean for class 0 (equation 4)\n",
    "\n",
    "        mu1 = (total_mean - mu_k) / w1  # mean for class 1 (equation 5) \n",
    "\n",
    "        # Between class variance\n",
    "        variance = w0 * w1 * (mu1 - mu0) ** 2  #(equation 14)\n",
    "\n",
    "        if variance > max_variance: # find the maximum variance and corresponding threshold\n",
    "            max_variance = variance\n",
    "            threshold = i\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b0210",
   "metadata": {},
   "source": [
    "## cleaning/denoising algorithm\n",
    "\n",
    "after reading the description i realised that you could basically just perform a convolution operation using a kernel to sum, and then choosing the class of the given pixel by using the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee886dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising_algorithm(image, vote_threshold):\n",
    "\n",
    "  kernel = np.array([\n",
    "        [1, 1, 1],\n",
    "        [1, 0, 1],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "\n",
    "  votes = convolve2d(image, kernel, mode='same', boundary='fill', fillvalue=0)  # returns the votes for each pixel\n",
    "\n",
    "  denoised_image = (votes >= vote_threshold).astype(np.uint8)\n",
    "\n",
    "  return denoised_image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fcceae",
   "metadata": {},
   "source": [
    "## Feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grey_scale_conversion(image):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d63ac8",
   "metadata": {},
   "source": [
    "# Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df312d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba8a213a",
   "metadata": {},
   "source": [
    "## Perform segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VIP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6600498",
   "metadata": {},
   "source": [
    "###Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27771f0",
   "metadata": {},
   "source": [
    "Imports + reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14a4cfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MiniBatchKMeans\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs):\n",
    "        return x\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdcd1d",
   "metadata": {},
   "source": [
    "Configuration (edit paths and parameters here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- REQUIRED: set this to your local Caltech101 folder ----\n",
    "# This should point to the directory that contains the category folders.\n",
    "# Typical example:\n",
    "# DATASET_ROOT = Path(\"/path/to/101_ObjectCategories\")\n",
    "DATASET_ROOT = Path(\"CHANGE_ME/101_ObjectCategories\")\n",
    "\n",
    "# Choose categories explicitly, or set to None to auto-pick the first N (excluding BACKGROUND_Google).\n",
    "CATEGORIES = None\n",
    "N_CATEGORIES_AUTO = 5\n",
    "\n",
    "# Training/test split per category:\n",
    "# - If you use a small subset, a common setting is 10 train + 10 test per category.\n",
    "TRAIN_PER_CLASS = 10\n",
    "TEST_PER_CLASS = 10\n",
    "\n",
    "# Codebook size (k in k-means). Typical values: 200–500 for small setups.\n",
    "K = 400\n",
    "\n",
    "# Descriptor sampling for k-means training to keep runtime reasonable:\n",
    "DESC_PER_IMAGE_FOR_KMEANS = 300         # take at most this many descriptors from each train image for k-means fitting\n",
    "MAX_KMEANS_DESCRIPTORS = 120_000        # global cap for k-means fitting matrix\n",
    "\n",
    "# Optional speed knob: cap descriptors used when building each image histogram (None = use all)\n",
    "MAX_DESC_PER_IMAGE_FOR_HIST = None\n",
    "\n",
    "# Normalize histograms (recommended)\n",
    "L1_NORMALIZE_HIST = True\n",
    "\n",
    "# Output artifacts\n",
    "OUTPUT_DIR = Path(\"./exercise1_codebook_artifacts\")\n",
    "DESC_CACHE_DIR = OUTPUT_DIR / \"desc_cache\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DESC_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATASET_ROOT:\", DATASET_ROOT.resolve())\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435d13c",
   "metadata": {},
   "source": [
    "Dataset utilities (scan categories, split train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "def list_categories(dataset_root: Path):\n",
    "    if not dataset_root.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"DATASET_ROOT does not exist: {dataset_root}\\n\"\n",
    "            \"Set DATASET_ROOT to the Caltech101 category folder (e.g., .../101_ObjectCategories).\"\n",
    "        )\n",
    "    cats = [p.name for p in dataset_root.iterdir() if p.is_dir()]\n",
    "    # Common to exclude this category in Caltech101:\n",
    "    cats = [c for c in cats if c.lower() != \"background_google\"]\n",
    "    cats.sort()\n",
    "    return cats\n",
    "\n",
    "def list_images_for_category(cat_dir: Path):\n",
    "    files = [p for p in cat_dir.iterdir() if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def split_train_test(files, n_train, n_test, seed):\n",
    "    files = list(files)\n",
    "    local_rng = np.random.default_rng(seed)\n",
    "    local_rng.shuffle(files)\n",
    "    n_total = min(len(files), n_train + n_test)\n",
    "    files = files[:n_total]\n",
    "    train_files = files[:min(n_train, len(files))]\n",
    "    test_files = files[min(n_train, len(files)):min(n_train + n_test, len(files))]\n",
    "    # Ensure equal sizes if possible\n",
    "    m = min(len(train_files), len(test_files))\n",
    "    return train_files[:m], test_files[:m]\n",
    "\n",
    "all_categories = list_categories(DATASET_ROOT)\n",
    "if CATEGORIES is None:\n",
    "    CATEGORIES = all_categories[:N_CATEGORIES_AUTO]\n",
    "else:\n",
    "    missing = [c for c in CATEGORIES if (DATASET_ROOT / c).is_dir() is False]\n",
    "    if missing:\n",
    "        raise ValueError(f\"These categories were not found under DATASET_ROOT: {missing}\")\n",
    "\n",
    "print(f\"Using {len(CATEGORIES)} categories:\")\n",
    "print(CATEGORIES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5660d7",
   "metadata": {},
   "source": [
    "Build a metadata table for the split (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for ci, cat in enumerate(CATEGORIES):\n",
    "    cat_dir = DATASET_ROOT / cat\n",
    "    files = list_images_for_category(cat_dir)\n",
    "    train_files, test_files = split_train_test(\n",
    "        files, n_train=TRAIN_PER_CLASS, n_test=TEST_PER_CLASS, seed=RANDOM_SEED + ci\n",
    "    )\n",
    "\n",
    "    for p in train_files:\n",
    "        rows.append({\"path\": p, \"category\": cat, \"split\": \"train\"})\n",
    "    for p in test_files:\n",
    "        rows.append({\"path\": p, \"category\": cat, \"split\": \"test\"})\n",
    "\n",
    "meta = pd.DataFrame(rows)\n",
    "if meta.empty:\n",
    "    raise RuntimeError(\"No images found. Check DATASET_ROOT and expected folder structure.\")\n",
    "\n",
    "print(meta.groupby([\"category\", \"split\"]).size().unstack(fill_value=0))\n",
    "meta.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba07c50",
   "metadata": {},
   "source": [
    "SIFT extractor + descriptor caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sift():\n",
    "    # OpenCV builds differ; try the standard entry point first.\n",
    "    if hasattr(cv2, \"SIFT_create\"):\n",
    "        return cv2.SIFT_create()\n",
    "    # Older fallback (opencv-contrib)\n",
    "    if hasattr(cv2, \"xfeatures2d\") and hasattr(cv2.xfeatures2d, \"SIFT_create\"):\n",
    "        return cv2.xfeatures2d.SIFT_create()\n",
    "    raise RuntimeError(\n",
    "        \"SIFT is not available in your OpenCV build.\\n\"\n",
    "        \"Install opencv-contrib-python (not just opencv-python), then restart the kernel.\"\n",
    "    )\n",
    "\n",
    "sift = create_sift()\n",
    "\n",
    "def _cache_path_for_image(image_path: Path) -> Path:\n",
    "    # Hash absolute path so the cache name is safe and unique.\n",
    "    h = hashlib.md5(str(image_path.resolve()).encode(\"utf-8\")).hexdigest()\n",
    "    return DESC_CACHE_DIR / f\"{h}.npz\"\n",
    "\n",
    "def read_gray(image_path: Path) -> np.ndarray:\n",
    "    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Failed to read image: {image_path}\")\n",
    "    return img\n",
    "\n",
    "def get_sift_descriptors(image_path: Path, use_cache: bool = True) -> np.ndarray:\n",
    "    cpath = _cache_path_for_image(image_path)\n",
    "    if use_cache and cpath.exists():\n",
    "        data = np.load(cpath)\n",
    "        return data[\"desc\"].astype(np.float32, copy=False)\n",
    "\n",
    "    img = read_gray(image_path)\n",
    "    kps, desc = sift.detectAndCompute(img, None)\n",
    "\n",
    "    if desc is None:\n",
    "        desc = np.empty((0, 128), dtype=np.float32)\n",
    "    else:\n",
    "        desc = desc.astype(np.float32, copy=False)\n",
    "\n",
    "    if use_cache:\n",
    "        np.savez_compressed(cpath, desc=desc)\n",
    "    return desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fabfb4",
   "metadata": {},
   "source": [
    "Collect descriptors from training images for k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08029ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = meta[meta[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "train_paths = train_meta[\"path\"].tolist()\n",
    "\n",
    "desc_blocks = []\n",
    "total_added = 0\n",
    "\n",
    "for p in tqdm(train_paths, desc=\"Extracting SIFT (train) for k-means\"):\n",
    "    desc = get_sift_descriptors(p, use_cache=True)\n",
    "    if desc.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    # Sample per-image for k-means fitting\n",
    "    if desc.shape[0] > DESC_PER_IMAGE_FOR_KMEANS:\n",
    "        idx = rng.choice(desc.shape[0], size=DESC_PER_IMAGE_FOR_KMEANS, replace=False)\n",
    "        desc = desc[idx]\n",
    "\n",
    "    desc_blocks.append(desc)\n",
    "    total_added += desc.shape[0]\n",
    "\n",
    "X = np.vstack(desc_blocks) if desc_blocks else np.empty((0, 128), dtype=np.float32)\n",
    "print(\"Descriptors collected for k-means:\", X.shape)\n",
    "\n",
    "if X.shape[0] == 0:\n",
    "    raise RuntimeError(\"No SIFT descriptors found in training images. Try different categories or check images.\")\n",
    "\n",
    "# Global cap\n",
    "if X.shape[0] > MAX_KMEANS_DESCRIPTORS:\n",
    "    idx = rng.choice(X.shape[0], size=MAX_KMEANS_DESCRIPTORS, replace=False)\n",
    "    X = X[idx]\n",
    "    print(\"After global cap:\", X.shape)\n",
    "\n",
    "X = X.astype(np.float32, copy=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2cd628",
   "metadata": {},
   "source": [
    "Fit the visual-word codebook (k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b34e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniBatchKMeans is typically much faster than full KMeans for large descriptor sets.\n",
    "kmeans_kwargs = dict(\n",
    "    n_clusters=K,\n",
    "    random_state=RANDOM_SEED,\n",
    "    batch_size=4096,\n",
    "    max_iter=200,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "try:\n",
    "    kmeans = MiniBatchKMeans(**kmeans_kwargs, n_init=\"auto\")\n",
    "except TypeError:\n",
    "    # For older scikit-learn versions\n",
    "    kmeans = MiniBatchKMeans(**kmeans_kwargs, n_init=3)\n",
    "\n",
    "kmeans.fit(X)\n",
    "\n",
    "print(\"Codebook learned.\")\n",
    "print(\"Cluster centers shape:\", kmeans.cluster_centers_.shape)\n",
    "print(\"Inertia:\", float(kmeans.inertia_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd2720",
   "metadata": {},
   "source": [
    "Build BoW histogram for each training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_histogram(desc: np.ndarray, kmeans_model: MiniBatchKMeans, k: int, l1_normalize: bool = True) -> np.ndarray:\n",
    "    hist = np.zeros((k,), dtype=np.float32)\n",
    "    if desc is None or desc.shape[0] == 0:\n",
    "        return hist\n",
    "\n",
    "    words = kmeans_model.predict(desc)  # nearest centroid index for each descriptor\n",
    "    hist = np.bincount(words, minlength=k).astype(np.float32)\n",
    "\n",
    "    if l1_normalize:\n",
    "        s = hist.sum()\n",
    "        if s > 0:\n",
    "            hist /= s\n",
    "    return hist\n",
    "\n",
    "bows = []\n",
    "n_empty = 0\n",
    "\n",
    "for p in tqdm(train_paths, desc=\"Building BoW histograms (train)\"):\n",
    "    desc = get_sift_descriptors(p, use_cache=True)\n",
    "\n",
    "    if desc.shape[0] == 0:\n",
    "        n_empty += 1\n",
    "        bows.append(np.zeros((K,), dtype=np.float32))\n",
    "        continue\n",
    "\n",
    "    if MAX_DESC_PER_IMAGE_FOR_HIST is not None and desc.shape[0] > MAX_DESC_PER_IMAGE_FOR_HIST:\n",
    "        idx = rng.choice(desc.shape[0], size=MAX_DESC_PER_IMAGE_FOR_HIST, replace=False)\n",
    "        desc = desc[idx]\n",
    "\n",
    "    bows.append(bow_histogram(desc, kmeans, K, l1_normalize=L1_NORMALIZE_HIST))\n",
    "\n",
    "train_meta = train_meta.copy()\n",
    "train_meta[\"bow\"] = bows\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Training images:\", len(train_meta))\n",
    "print(\"Images with 0 descriptors:\", n_empty)\n",
    "train_meta.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9aee8",
   "metadata": {},
   "source": [
    "Save outputs (codebook + training BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save k-means model (contains the codebook centers)\n",
    "codebook_path = OUTPUT_DIR / f\"codebook_k{K}_minibatchkmeans.joblib\"\n",
    "joblib.dump(kmeans, codebook_path)\n",
    "\n",
    "# Save centers explicitly as numpy (optional but convenient)\n",
    "centers_path = OUTPUT_DIR / f\"codebook_centers_k{K}.npy\"\n",
    "np.save(centers_path, kmeans.cluster_centers_.astype(np.float32))\n",
    "\n",
    "# Save BoW matrix + metadata for training images\n",
    "B = np.vstack(train_meta[\"bow\"].to_numpy()).astype(np.float32)\n",
    "paths = train_meta[\"path\"].astype(str).to_numpy()\n",
    "labels = train_meta[\"category\"].to_numpy()\n",
    "\n",
    "bow_path = OUTPUT_DIR / f\"train_bow_k{K}.npz\"\n",
    "np.savez_compressed(bow_path, B=B, paths=paths, labels=labels)\n",
    "\n",
    "# Also save a simple CSV (without the raw vectors) for inspection\n",
    "csv_path = OUTPUT_DIR / f\"train_meta_k{K}.csv\"\n",
    "train_meta.drop(columns=[\"bow\"]).to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", codebook_path)\n",
    "print(\" -\", centers_path)\n",
    "print(\" -\", bow_path)\n",
    "print(\" -\", csv_path)\n",
    "print(\"BoW matrix shape:\", B.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52860c",
   "metadata": {},
   "source": [
    "Quick sanity checks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d18d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few histograms sum to 1 (if L1 normalized)\n",
    "sums = B.sum(axis=1)\n",
    "print(\"Histogram sum stats:\")\n",
    "print(\" min:\", float(sums.min()), \" max:\", float(sums.max()), \" mean:\", float(sums.mean()))\n",
    "\n",
    "# Count per-class training images\n",
    "print(\"\\nTrain images per class:\")\n",
    "print(pd.Series(labels).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0bcc6",
   "metadata": {},
   "source": [
    "###Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c01ae",
   "metadata": {},
   "source": [
    "Prerequisite check + (optional) load codebook if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# Expect these from Exercise 1:\n",
    "# - meta : DataFrame with columns [\"path\", \"category\", \"split\"]\n",
    "# - K : codebook size (int)\n",
    "# - OUTPUT_DIR : output folder Path\n",
    "# - get_sift_descriptors : function (path -> (N,128) float32)\n",
    "# - kmeans : fitted (MiniBatch)KMeans with .predict()\n",
    "\n",
    "missing = []\n",
    "for name in [\"meta\", \"K\", \"OUTPUT_DIR\"]:\n",
    "    if name not in globals():\n",
    "        missing.append(name)\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"Missing required variables from Exercise 1: \"\n",
    "        + \", \".join(missing)\n",
    "        + \"\\nRun the Exercise 1 cells first (dataset split + codebook learning).\"\n",
    "    )\n",
    "\n",
    "# Load codebook model from disk if not present in memory\n",
    "if \"kmeans\" not in globals():\n",
    "    codebook_path = Path(OUTPUT_DIR) / f\"codebook_k{K}_minibatchkmeans.joblib\"\n",
    "    if not codebook_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find saved codebook at: {codebook_path}\\n\"\n",
    "            \"Either re-run Exercise 1 (k-means fitting) or update OUTPUT_DIR/K.\"\n",
    "        )\n",
    "    kmeans = joblib.load(codebook_path)\n",
    "\n",
    "# Ensure descriptor extraction exists; if not, stop with a clear message\n",
    "if \"get_sift_descriptors\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"get_sift_descriptors(...) is not defined.\\n\"\n",
    "        \"Run the Exercise 1 cell that defines SIFT extraction + caching.\"\n",
    "    )\n",
    "\n",
    "print(\"OK: meta/K/OUTPUT_DIR present, and codebook (kmeans) is available.\")\n",
    "print(\"Indexing images:\", len(meta), \"| K =\", K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0607b8",
   "metadata": {},
   "source": [
    "BoW construction (counts + L1-normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36898e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_counts_and_l1(desc: np.ndarray, kmeans_model, k: int):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      hist_counts: (k,) float32\n",
    "      hist_l1:     (k,) float32 (L1-normalized, or all zeros if no descriptors)\n",
    "    \"\"\"\n",
    "    hist_counts = np.zeros((k,), dtype=np.float32)\n",
    "\n",
    "    if desc is None or desc.shape[0] == 0:\n",
    "        return hist_counts, hist_counts.copy()\n",
    "\n",
    "    word_ids = kmeans_model.predict(desc)  # nearest centroid index per descriptor\n",
    "    hist_counts = np.bincount(word_ids, minlength=k).astype(np.float32)\n",
    "\n",
    "    hist_l1 = hist_counts.copy()\n",
    "    s = float(hist_l1.sum())\n",
    "    if s > 0:\n",
    "        hist_l1 /= s\n",
    "    return hist_counts, hist_l1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09a0f7",
   "metadata": {},
   "source": [
    "Compute BoW for every image (train + test) and build the index table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "index_df = meta.copy().reset_index(drop=True)\n",
    "index_df[\"filename\"] = index_df[\"path\"].apply(lambda p: Path(p).name)\n",
    "\n",
    "bows_counts = []\n",
    "bows_l1 = []\n",
    "n_empty = 0\n",
    "\n",
    "for p in tqdm(index_df[\"path\"].tolist(), desc=\"Indexing images (SIFT -> BoW)\"):\n",
    "    desc = get_sift_descriptors(Path(p), use_cache=True)\n",
    "\n",
    "    if desc.shape[0] == 0:\n",
    "        n_empty += 1\n",
    "\n",
    "    hc, hl1 = bow_counts_and_l1(desc, kmeans, K)\n",
    "    bows_counts.append(hc)\n",
    "    bows_l1.append(hl1)\n",
    "\n",
    "index_df[\"bow_counts\"] = bows_counts\n",
    "index_df[\"bow_l1\"] = bows_l1\n",
    "\n",
    "print(\"Done indexing.\")\n",
    "print(\"Images:\", len(index_df))\n",
    "print(\"Images with 0 descriptors:\", n_empty)\n",
    "\n",
    "index_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5ad21",
   "metadata": {},
   "source": [
    "Save the index (metadata table + feature matrices) for repeated reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Stack to matrices for fast retrieval later\n",
    "B_counts = np.vstack(index_df[\"bow_counts\"].to_numpy()).astype(np.float32)\n",
    "B_l1 = np.vstack(index_df[\"bow_l1\"].to_numpy()).astype(np.float32)\n",
    "\n",
    "paths = index_df[\"path\"].astype(str).to_numpy()\n",
    "filenames = index_df[\"filename\"].astype(str).to_numpy()\n",
    "labels = index_df[\"category\"].astype(str).to_numpy()\n",
    "splits = index_df[\"split\"].astype(str).to_numpy()\n",
    "\n",
    "# Save metadata (human-readable)\n",
    "index_meta_path = OUTPUT_DIR / f\"index_meta_k{K}.csv\"\n",
    "index_df.drop(columns=[\"bow_counts\", \"bow_l1\"]).to_csv(index_meta_path, index=False)\n",
    "\n",
    "# Save features (compact + fast to load)\n",
    "index_features_path = OUTPUT_DIR / f\"index_features_k{K}.npz\"\n",
    "np.savez_compressed(\n",
    "    index_features_path,\n",
    "    B_counts=B_counts,\n",
    "    B_l1=B_l1,\n",
    "    paths=paths,\n",
    "    filenames=filenames,\n",
    "    labels=labels,\n",
    "    splits=splits,\n",
    "    K=np.array([K], dtype=np.int32),\n",
    ")\n",
    "\n",
    "# Optional: save a single pickle/joblib if you prefer one file (not required)\n",
    "index_joblib_path = OUTPUT_DIR / f\"index_table_k{K}.joblib\"\n",
    "joblib.dump(index_df, index_joblib_path)\n",
    "\n",
    "print(\"Saved indexing artifacts:\")\n",
    "print(\" -\", index_meta_path)\n",
    "print(\" -\", index_features_path)\n",
    "print(\" -\", index_joblib_path)\n",
    "print(\"B_counts shape:\", B_counts.shape, \"| B_l1 shape:\", B_l1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af5480",
   "metadata": {},
   "source": [
    "Convenience loader (use in Exercise 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26013e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_table(output_dir: Path, k: int):\n",
    "    output_dir = Path(output_dir)\n",
    "    meta_path = output_dir / f\"index_meta_k{k}.csv\"\n",
    "    feat_path = output_dir / f\"index_features_k{k}.npz\"\n",
    "\n",
    "    if not meta_path.exists() or not feat_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"Missing index files.\\n\"\n",
    "            f\"Expected:\\n  {meta_path}\\n  {feat_path}\"\n",
    "        )\n",
    "\n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "    z = np.load(feat_path, allow_pickle=False)\n",
    "    B_counts = z[\"B_counts\"].astype(np.float32, copy=False)\n",
    "    B_l1 = z[\"B_l1\"].astype(np.float32, copy=False)\n",
    "\n",
    "    return meta_df, B_counts, B_l1\n",
    "\n",
    "# Example usage:\n",
    "meta_df_loaded, B_counts_loaded, B_l1_loaded = load_index_table(OUTPUT_DIR, K)\n",
    "print(meta_df_loaded.head())\n",
    "print(\"Loaded shapes:\", B_counts_loaded.shape, B_l1_loaded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda88eb",
   "metadata": {},
   "source": [
    "###Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698772b9",
   "metadata": {},
   "source": [
    "Cell 1 — Load the saved index (or reuse index_df if already in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c99d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Requires from previous exercises:\n",
    "# - OUTPUT_DIR (Path or str)\n",
    "# - K (int)\n",
    "\n",
    "if \"OUTPUT_DIR\" not in globals() or \"K\" not in globals():\n",
    "    raise RuntimeError(\"Missing OUTPUT_DIR or K. Run Exercise 1/2 cells first.\")\n",
    "\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "\n",
    "def load_index_features(output_dir: Path, k: int):\n",
    "    feat_path = output_dir / f\"index_features_k{k}.npz\"\n",
    "    meta_path = output_dir / f\"index_meta_k{k}.csv\"\n",
    "\n",
    "    if not feat_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {feat_path}. Run Exercise 2 saving cell first.\")\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {meta_path}. Run Exercise 2 saving cell first.\")\n",
    "\n",
    "    z = np.load(feat_path, allow_pickle=False)\n",
    "    B_counts = z[\"B_counts\"].astype(np.float32, copy=False)\n",
    "    B_l1 = z[\"B_l1\"].astype(np.float32, copy=False)\n",
    "    paths = z[\"paths\"].astype(str)\n",
    "    labels = z[\"labels\"].astype(str)\n",
    "    splits = z[\"splits\"].astype(str)\n",
    "    meta_df = pd.read_csv(meta_path)\n",
    "\n",
    "    return meta_df, B_counts, B_l1, paths, labels, splits\n",
    "\n",
    "# Prefer in-memory index_df if present and well-formed, otherwise load from disk.\n",
    "use_in_memory = (\"index_df\" in globals() and isinstance(index_df, pd.DataFrame)\n",
    "                 and \"bow_counts\" in index_df.columns and \"bow_l1\" in index_df.columns)\n",
    "\n",
    "if use_in_memory:\n",
    "    paths_all = index_df[\"path\"].astype(str).to_numpy()\n",
    "    labels_all = index_df[\"category\"].astype(str).to_numpy()\n",
    "    splits_all = index_df[\"split\"].astype(str).to_numpy()\n",
    "    B_counts_all = np.vstack(index_df[\"bow_counts\"].to_numpy()).astype(np.float32)\n",
    "    B_l1_all = np.vstack(index_df[\"bow_l1\"].to_numpy()).astype(np.float32)\n",
    "    meta_df = index_df.drop(columns=[\"bow_counts\", \"bow_l1\"]).copy()\n",
    "else:\n",
    "    meta_df, B_counts_all, B_l1_all, paths_all, labels_all, splits_all = load_index_features(OUTPUT_DIR, K)\n",
    "\n",
    "train_mask = (splits_all == \"train\")\n",
    "test_mask  = (splits_all == \"test\")\n",
    "\n",
    "print(\"All images:\", len(paths_all))\n",
    "print(\"Train:\", int(train_mask.sum()), \"| Test:\", int(test_mask.sum()), \"| K:\", K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0003c",
   "metadata": {},
   "source": [
    "Prepare representations used by the similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd09856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training database (used in both experiments)\n",
    "D_train_counts = B_counts_all[train_mask]\n",
    "D_train_l1     = B_l1_all[train_mask]\n",
    "D_train_paths  = paths_all[train_mask]\n",
    "D_train_labels = labels_all[train_mask]\n",
    "\n",
    "# Queries for the two experiments\n",
    "Q_train_counts = D_train_counts\n",
    "Q_train_l1     = D_train_l1\n",
    "Q_train_paths  = D_train_paths\n",
    "Q_train_labels = D_train_labels\n",
    "\n",
    "Q_test_counts  = B_counts_all[test_mask]\n",
    "Q_test_l1      = B_l1_all[test_mask]\n",
    "Q_test_paths   = paths_all[test_mask]\n",
    "Q_test_labels  = labels_all[test_mask]\n",
    "\n",
    "# 1) Common-words presence vectors (boolean)\n",
    "D_train_bin = (D_train_counts > 0)\n",
    "Q_train_bin = (Q_train_counts > 0)\n",
    "Q_test_bin  = (Q_test_counts  > 0)\n",
    "\n",
    "# 2) Bhattacharyya: sqrt of L1 histograms (still valid as long as entries are nonnegative)\n",
    "D_train_sqrt = np.sqrt(np.clip(D_train_l1, 0.0, None))\n",
    "Q_train_sqrt = np.sqrt(np.clip(Q_train_l1, 0.0, None))\n",
    "Q_test_sqrt  = np.sqrt(np.clip(Q_test_l1,  0.0, None))\n",
    "\n",
    "# 3) TF-IDF (IDF computed on TRAINING set only)\n",
    "df = D_train_bin.sum(axis=0).astype(np.float32)     # document frequency per word\n",
    "N  = float(D_train_bin.shape[0])\n",
    "idf = (np.log((N + 1.0) / (df + 1.0)) + 1.0).astype(np.float32)  # (K,)\n",
    "\n",
    "def tfidf_l2_normalize(B_l1: np.ndarray, idf: np.ndarray) -> np.ndarray:\n",
    "    V = (B_l1 * idf[None, :]).astype(np.float32, copy=False)\n",
    "    norms = np.linalg.norm(V, axis=1, keepdims=True)\n",
    "    mask = norms[:, 0] > 0\n",
    "    Vn = np.zeros_like(V, dtype=np.float32)\n",
    "    Vn[mask] = V[mask] / norms[mask]\n",
    "    return Vn\n",
    "\n",
    "D_train_tfidf = tfidf_l2_normalize(D_train_l1, idf)\n",
    "Q_train_tfidf = D_train_tfidf\n",
    "Q_test_tfidf  = tfidf_l2_normalize(Q_test_l1, idf)\n",
    "\n",
    "print(\"Prepared: common-words, Bhattacharyya, TF-IDF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d01cf4",
   "metadata": {},
   "source": [
    "Similarity score functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_common_words(Q_bin: np.ndarray, D_bin: np.ndarray) -> np.ndarray:\n",
    "    # Similarity = count of indices where both have presence True\n",
    "    # bool @ bool -> int; cast to float for uniform downstream handling\n",
    "    return (Q_bin @ D_bin.T).astype(np.float32)\n",
    "\n",
    "def scores_cosine(Q_l2: np.ndarray, D_l2: np.ndarray) -> np.ndarray:\n",
    "    # Assumes rows are L2-normalized; similarity is dot product\n",
    "    return (Q_l2 @ D_l2.T).astype(np.float32)\n",
    "\n",
    "def scores_bhattacharyya(Q_sqrt: np.ndarray, D_sqrt: np.ndarray) -> np.ndarray:\n",
    "    # Bhattacharyya coefficient for L1-normalized histograms:\n",
    "    # BC(p,q) = sum_j sqrt(p_j q_j) = <sqrt(p), sqrt(q)>\n",
    "    return (Q_sqrt @ D_sqrt.T).astype(np.float32)\n",
    "\n",
    "def prepare_kl_db(D_l1: np.ndarray, eps: float = 1e-10):\n",
    "    D = (D_l1.astype(np.float32, copy=False) + np.float32(eps))\n",
    "    D /= D.sum(axis=1, keepdims=True)\n",
    "    logD = np.log(D)\n",
    "    row_D_logD = np.sum(D * logD, axis=1)  # shape (n_db,)\n",
    "    return D, logD, row_D_logD\n",
    "\n",
    "def scores_sym_kl(Q_l1: np.ndarray, D_prepared, eps: float = 1e-10, batch_size: int = 128) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns similarity = - symmetric KL distance.\n",
    "    \"\"\"\n",
    "    D, logD, row_D_logD = D_prepared\n",
    "    n_q = Q_l1.shape[0]\n",
    "    n_db = D.shape[0]\n",
    "    out = np.empty((n_q, n_db), dtype=np.float32)\n",
    "\n",
    "    for start in range(0, n_q, batch_size):\n",
    "        end = min(start + batch_size, n_q)\n",
    "        Q = (Q_l1[start:end].astype(np.float32, copy=False) + np.float32(eps))\n",
    "        Q /= Q.sum(axis=1, keepdims=True)\n",
    "        logQ = np.log(Q)\n",
    "\n",
    "        # KL(Q || D): const(Q) - logD @ Q^T\n",
    "        const = np.sum(Q * logQ, axis=1)  # (b,)\n",
    "        term_logD = (logD @ Q.T).T        # (b, n_db)\n",
    "        kl_qd = const[:, None] - term_logD\n",
    "\n",
    "        # KL(D || Q): row(D log D) - D @ logQ^T\n",
    "        term_DlogQ = (D @ logQ.T).T       # (b, n_db)\n",
    "        kl_dq = row_D_logD[None, :] - term_DlogQ\n",
    "\n",
    "        sym = 0.5 * (kl_qd + kl_dq)       # (b, n_db)\n",
    "        out[start:end] = (-sym).astype(np.float32)  # similarity = -distance\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80296578",
   "metadata": {},
   "source": [
    "Ranking + metrics (MRR and Top-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f719afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(scores: np.ndarray,\n",
    "                       query_labels: np.ndarray,\n",
    "                       db_labels: np.ndarray,\n",
    "                       topk: int = 3,\n",
    "                       exclude_self_diag: bool = False):\n",
    "    \"\"\"\n",
    "    scores: (n_q, n_db), higher is better\n",
    "    \"\"\"\n",
    "    S = scores.copy()\n",
    "    if exclude_self_diag:\n",
    "        if S.shape[0] != S.shape[1]:\n",
    "            raise ValueError(\"exclude_self_diag=True requires a square score matrix.\")\n",
    "        np.fill_diagonal(S, -np.inf)\n",
    "\n",
    "    n_q = S.shape[0]\n",
    "    rr = np.zeros((n_q,), dtype=np.float32)\n",
    "    topk_hit = np.zeros((n_q,), dtype=np.float32)\n",
    "    first_ranks = np.full((n_q,), np.inf, dtype=np.float32)\n",
    "\n",
    "    for i in range(n_q):\n",
    "        order = np.argsort(-S[i])  # descending similarity\n",
    "        lbls = db_labels[order]\n",
    "\n",
    "        hits = np.where(lbls == query_labels[i])[0]\n",
    "        if hits.size > 0:\n",
    "            rank = float(hits[0] + 1)  # 1-indexed\n",
    "            first_ranks[i] = rank\n",
    "            rr[i] = 1.0 / rank\n",
    "\n",
    "        topk_hit[i] = 1.0 if np.any(lbls[:topk] == query_labels[i]) else 0.0\n",
    "\n",
    "    mrr = float(rr.mean()) if n_q > 0 else 0.0\n",
    "    topk_pct = float(100.0 * topk_hit.mean()) if n_q > 0 else 0.0\n",
    "    return {\n",
    "        \"MRR\": mrr,\n",
    "        \"Top3_%\": topk_pct,\n",
    "        \"ranks\": first_ranks,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b3e28",
   "metadata": {},
   "source": [
    "Run both required experiments for multiple similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute KL DB (training set only)\n",
    "D_train_kl_prepared = prepare_kl_db(D_train_l1, eps=1e-10)\n",
    "\n",
    "results = []\n",
    "\n",
    "# ---- 1) Common words ----\n",
    "S_train = scores_common_words(Q_train_bin, D_train_bin)\n",
    "m_train = evaluate_retrieval(S_train, Q_train_labels, D_train_labels, topk=3, exclude_self_diag=True)\n",
    "\n",
    "S_test = scores_common_words(Q_test_bin, D_train_bin)\n",
    "m_test = evaluate_retrieval(S_test, Q_test_labels, D_train_labels, topk=3, exclude_self_diag=False)\n",
    "\n",
    "results.append({\n",
    "    \"measure\": \"common_words\",\n",
    "    \"train_MRR\": m_train[\"MRR\"], \"train_Top3_%\": m_train[\"Top3_%\"],\n",
    "    \"test_MRR\":  m_test[\"MRR\"],  \"test_Top3_%\":  m_test[\"Top3_%\"],\n",
    "})\n",
    "\n",
    "# ---- 2) TF-IDF cosine ----\n",
    "S_train = scores_cosine(Q_train_tfidf, D_train_tfidf)\n",
    "m_train = evaluate_retrieval(S_train, Q_train_labels, D_train_labels, topk=3, exclude_self_diag=True)\n",
    "\n",
    "S_test = scores_cosine(Q_test_tfidf, D_train_tfidf)\n",
    "m_test = evaluate_retrieval(S_test, Q_test_labels, D_train_labels, topk=3, exclude_self_diag=False)\n",
    "\n",
    "results.append({\n",
    "    \"measure\": \"tfidf_cosine\",\n",
    "    \"train_MRR\": m_train[\"MRR\"], \"train_Top3_%\": m_train[\"Top3_%\"],\n",
    "    \"test_MRR\":  m_test[\"MRR\"],  \"test_Top3_%\":  m_test[\"Top3_%\"],\n",
    "})\n",
    "\n",
    "# ---- 3) Bhattacharyya coefficient ----\n",
    "S_train = scores_bhattacharyya(Q_train_sqrt, D_train_sqrt)\n",
    "m_train = evaluate_retrieval(S_train, Q_train_labels, D_train_labels, topk=3, exclude_self_diag=True)\n",
    "\n",
    "S_test = scores_bhattacharyya(Q_test_sqrt, D_train_sqrt)\n",
    "m_test = evaluate_retrieval(S_test, Q_test_labels, D_train_labels, topk=3, exclude_self_diag=False)\n",
    "\n",
    "results.append({\n",
    "    \"measure\": \"bhattacharyya\",\n",
    "    \"train_MRR\": m_train[\"MRR\"], \"train_Top3_%\": m_train[\"Top3_%\"],\n",
    "    \"test_MRR\":  m_test[\"MRR\"],  \"test_Top3_%\":  m_test[\"Top3_%\"],\n",
    "})\n",
    "\n",
    "# ---- 4) Symmetric KL (similarity = -distance) ----\n",
    "S_train = scores_sym_kl(Q_train_l1, D_train_kl_prepared, eps=1e-10, batch_size=128)\n",
    "m_train = evaluate_retrieval(S_train, Q_train_labels, D_train_labels, topk=3, exclude_self_diag=True)\n",
    "\n",
    "S_test = scores_sym_kl(Q_test_l1, D_train_kl_prepared, eps=1e-10, batch_size=128)\n",
    "m_test = evaluate_retrieval(S_test, Q_test_labels, D_train_labels, topk=3, exclude_self_diag=False)\n",
    "\n",
    "results.append({\n",
    "    \"measure\": \"sym_kl\",\n",
    "    \"train_MRR\": m_train[\"MRR\"], \"train_Top3_%\": m_train[\"Top3_%\"],\n",
    "    \"test_MRR\":  m_test[\"MRR\"],  \"test_Top3_%\":  m_test[\"Top3_%\"],\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"test_MRR\", ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2e211",
   "metadata": {},
   "source": [
    "Save the results table (for your report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv = OUTPUT_DIR / f\"exercise3_results_k{K}.csv\"\n",
    "results_df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7232f14",
   "metadata": {},
   "source": [
    "Visualize retrieval examples (optional, useful for the report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def _read_rgb(path: str):\n",
    "    im = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "    if im is None:\n",
    "        raise FileNotFoundError(f\"Could not read: {path}\")\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    return im\n",
    "\n",
    "def show_retrieval(query_path: str, query_label: str,\n",
    "                   db_paths: np.ndarray, db_labels: np.ndarray,\n",
    "                   scores_row: np.ndarray, topk: int = 5,\n",
    "                   title: str = \"\"):\n",
    "    order = np.argsort(-scores_row)[:topk]\n",
    "    fig = plt.figure(figsize=(3 * (topk + 1), 3))\n",
    "\n",
    "    # Query\n",
    "    ax = fig.add_subplot(1, topk + 1, 1)\n",
    "    ax.imshow(_read_rgb(query_path))\n",
    "    ax.set_title(f\"Query\\n{query_label}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Top-k results\n",
    "    for j, idx in enumerate(order, start=2):\n",
    "        ax = fig.add_subplot(1, topk + 1, j)\n",
    "        ax.imshow(_read_rgb(db_paths[idx]))\n",
    "        ax.set_title(f\"Rank {j-1}\\n{db_labels[idx]}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def get_score_row(metric: str, q_idx: int, split: str = \"test\"):\n",
    "    if split == \"test\":\n",
    "        q_path, q_lbl = Q_test_paths[q_idx], Q_test_labels[q_idx]\n",
    "        q_bin, q_tfidf, q_sqrt, q_l1 = Q_test_bin[q_idx:q_idx+1], Q_test_tfidf[q_idx:q_idx+1], Q_test_sqrt[q_idx:q_idx+1], Q_test_l1[q_idx:q_idx+1]\n",
    "    elif split == \"train\":\n",
    "        q_path, q_lbl = Q_train_paths[q_idx], Q_train_labels[q_idx]\n",
    "        q_bin, q_tfidf, q_sqrt, q_l1 = Q_train_bin[q_idx:q_idx+1], Q_train_tfidf[q_idx:q_idx+1], Q_train_sqrt[q_idx:q_idx+1], Q_train_l1[q_idx:q_idx+1]\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train' or 'test'\")\n",
    "\n",
    "    if metric == \"common_words\":\n",
    "        s = scores_common_words(q_bin, D_train_bin)[0]\n",
    "    elif metric == \"tfidf_cosine\":\n",
    "        s = scores_cosine(q_tfidf, D_train_tfidf)[0]\n",
    "    elif metric == \"bhattacharyya\":\n",
    "        s = scores_bhattacharyya(q_sqrt, D_train_sqrt)[0]\n",
    "    elif metric == \"sym_kl\":\n",
    "        s = scores_sym_kl(q_l1, D_train_kl_prepared, eps=1e-10, batch_size=1)[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "    return q_path, q_lbl, s\n",
    "\n",
    "# Example: show retrieval for a random test query using the best metric from results_df\n",
    "if len(Q_test_paths) > 0:\n",
    "    metric_best = results_df.loc[0, \"measure\"]\n",
    "    q_idx = 0  # change to any index in [0, len(Q_test_paths)-1]\n",
    "    q_path, q_lbl, srow = get_score_row(metric_best, q_idx, split=\"test\")\n",
    "    show_retrieval(q_path, q_lbl, D_train_paths, D_train_labels, srow, topk=5,\n",
    "                   title=f\"Test query retrieval using {metric_best}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2e737",
   "metadata": {},
   "source": [
    "Simple plot for comparing measures (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b28e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.bar(x - 0.2, results_df[\"train_MRR\"], width=0.4, label=\"Train retrieval MRR\")\n",
    "plt.bar(x + 0.2, results_df[\"test_MRR\"],  width=0.4, label=\"Test classification MRR\")\n",
    "plt.xticks(x, results_df[\"measure\"], rotation=20)\n",
    "plt.ylabel(\"MRR\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
